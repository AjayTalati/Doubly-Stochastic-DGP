{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive scale regression: \n",
    "**Warning:** this dataset will occupy 76GB on space your disk. Check that the download location is appropriate for data this size. You will also need about 16GB of memory to run this code. \n",
    "\n",
    "The taxi data set consists of 1.21 billion yellow taxi journeys in New York. We obtained the data from http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n",
    "\n",
    "The processing was as follows:\n",
    "- We extracted the following features: time of day; day of the week; day of the month; month; pick-up latitude and longitude; drop-off latitude and longitude; travel distance; journey time (the target)\n",
    "- We discarded journeys that are less than 10 s or greater than 5 h, or start/end outside the New York region, which we judge to have squared distance less than $5^o$ from the centre of New York\n",
    "- As we read in the data we calculated $\\sum x$ and $\\sum x^2$. These are in the file `taxi_data_stats.p`. We use these for normalizing the data. In the paper we normalise the outputs and restore the scaling, but here we use a mean function and set the variance accordingly. \n",
    "- We shuffled the entire data set (we used a machine with 224GB of memory to do this) and then split the data into 101 files each with $10^7$ lines. We use the first 100 chunks for training and final chunk for testing \n",
    "\n",
    "To use this data set managably on a standard machine we read in two chunks at a time, the second loading asynchronously as the first chunk is used for training. We have a special `DataHolder` class for this  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from GPflow.likelihoods import Gaussian\n",
    "from GPflow.kernels import RBF, White\n",
    "from GPflow.mean_functions import Constant, Zero\n",
    "from GPflow.svgp import SVGP\n",
    "from GPflow.param import DataHolder, Parentable\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from get_data import get_taxi_data, get_taxi_stats\n",
    "\n",
    "from threading import Thread\n",
    "from Queue import Queue\n",
    "\n",
    "from dgp import DGP\n",
    "import time\n",
    "\n",
    "data_path = '/mnt/' # requires 76GB of free space. Download size is approx 28GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrapper(func, arg, queue):\n",
    "    queue.put(func(arg))\n",
    "\n",
    "class TaxiData(DataHolder):\n",
    "    def __init__(self, minibatch_size=10000):\n",
    "        Parentable.__init__(self)\n",
    "        self._shape = [minibatch_size, 10]\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.counter = 0\n",
    "        self.chunk_counter = 0\n",
    "        \n",
    "        self.num_data = int(10**9)\n",
    "        self.chunk_size = int(10**7)\n",
    "        self.num_chunks = int(self.num_data/self.chunk_size)\n",
    "\n",
    "        self.X_mean, self.X_std = get_taxi_stats(data_path=data_path) \n",
    "\n",
    "        self.current_chunk = self.get_chunk(0) # get first chunk\n",
    "        self.chunk_counter += 1\n",
    "        self.start_get_chunk(self.chunk_counter) # start loading next one\n",
    "        \n",
    "    \n",
    "    def start_get_chunk(self, i):\n",
    "        self.next_chunk_queued = Queue() \n",
    "        Thread(target=wrapper, args=(self.get_chunk, i, \n",
    "                                     self.next_chunk_queued)).start()\n",
    "    \n",
    "    def get_chunk(self, i):\n",
    "        return self.whiten_X(get_taxi_data(i, data_path=data_path))\n",
    "    \n",
    "    def whiten_X(self, data):\n",
    "        X = data[:, :-1]\n",
    "        Xw = (X - self.X_mean)/self.X_std\n",
    "        return np.concatenate([Xw, data[:, -1, None]], 1)\n",
    "    \n",
    "    def _get_type(self):\n",
    "        return np.float64\n",
    "\n",
    "    def make_tf_array(self):\n",
    "        self._tf_array = tf.placeholder(dtype=self._get_type(),\n",
    "                                        shape=[None, self._shape[1]],\n",
    "                                        name=self.name)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        raise NotImplementedError #can't access this data directly \n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        return np.prod(self.shape)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "\n",
    "    def __str__(self, prepend='Data:'):\n",
    "        return prepend + \\\n",
    "               '\\033[1m' + self.name + '\\033[0m' + \\\n",
    "               '\\n Data much too large to print!' + \\\n",
    "               '\\n First 10 lines of current chunk are: ' + \\\n",
    "                '\\n' + str(self.current_chunk[:10, :])\n",
    "                \n",
    "    def update_feed_dict(self, key_dict, feed_dict):\n",
    "        if self.counter + self.minibatch_size > self.chunk_size:\n",
    "            self.current_chunk = self.next_chunk_queued.get()\n",
    "            self.chunk_counter = (self.chunk_counter + 1) % self.num_chunks\n",
    "            self.start_get_chunk(self.chunk_counter)\n",
    "            self.counter = 0     \n",
    "       \n",
    "        start = self.counter\n",
    "        end = self.counter + self.minibatch_size\n",
    "        \n",
    "        self.counter += self.minibatch_size\n",
    "        \n",
    "        feed_dict[key_dict[self]] = self.current_chunk[start:end, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_data = TaxiData()\n",
    "test_data = taxi_data.get_chunk(101)\n",
    "Ns = int(1e6)\n",
    "Xs, Ys = test_data[:Ns, :-1], test_data[:Ns, -1, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the $10^6$ from the first chunk to find the initial inducing locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = kmeans2(taxi_data.current_chunk[:int(1e6), :-1], 100, minit='points')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a single layer model we need to slightly modify the base SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MassiveDataSVGP(SVGP):\n",
    "    def __init__(self, dataholder, kernel, likelihood, Z, q_diag=False, whiten=True, num_latent=1, mean_function=Zero()):\n",
    "        SVGP.__init__(self, np.zeros((1, 9)), np.zeros((1, 1)), kernel, likelihood, Z, \n",
    "                      q_diag=q_diag, whiten=whiten, num_latent=num_latent)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        self.dataholder = dataholder\n",
    "        self.num_data = dataholder.num_data\n",
    "        \n",
    "    def build_likelihood(self):\n",
    "        self.X = self.dataholder[:, :-1]\n",
    "        self.Y = self.dataholder[:, -1, None]        \n",
    "        return SVGP.build_likelihood(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise variance on this set is sufficiently large it overflows the transform (we don't trust the behaviour of `tf.sotfplus` for large values). We make a new Gaussian likeliood with no transform. In this situation this should be pretty safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from GPflow.likelihoods import Likelihood\n",
    "# from GPflow.param import Param\n",
    "# class NoTransformGaussian(Gaussian):\n",
    "#     def __init__(self):\n",
    "#         Likelihood.__init__(self)\n",
    "#         self.variance = Param(1.0)\n",
    "np.seterr(over='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_mean, Y_std = np.average(taxi_data.current_chunk[:, -1]), np.std(taxi_data.current_chunk[:, -1])\n",
    "\n",
    "num_val = int(1e5)\n",
    "X_val, Y_val = taxi_data.current_chunk[:num_val, :-1], taxi_data.current_chunk[:num_val, -1, None]\n",
    "\n",
    "m_sgp = MassiveDataSVGP(taxi_data, RBF(9, ARD=True, variance=Y_std**2), Gaussian(), Z.copy(), \n",
    "                        mean_function=Constant(Y_mean.copy()))\n",
    "m_sgp.likelihood.variance = 0.1*Y_std**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some tools to assess the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/1000.), 1)\n",
    "    lik, sq_diff = [], []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "        l, sq = assess_model(model, X_batch, Y_batch)\n",
    "        lik.append(l)\n",
    "        sq_diff.append(sq)\n",
    "    lik = np.concatenate(lik, 0)\n",
    "    sq_diff = np.array(np.concatenate(sq_diff, 0), dtype=float)\n",
    "    return np.average(lik), np.average(sq_diff)**0.5\n",
    "\n",
    "def assess_single_layer(model, X_batch, Y_batch):\n",
    "    lik = model.predict_density(X_batch, Y_batch)\n",
    "    mean, var = model.predict_y(X_batch)\n",
    "    sq_diff = ((mean - Y_batch)**2)\n",
    "    return lik, sq_diff \n",
    "\n",
    "S = 100\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    lik = model.predict_density(X_batch, Y_batch, S)\n",
    "    mean_samples, var_samples = model.predict_y(X_batch, S)\n",
    "    mean = np.average(mean_samples, 0)\n",
    "    sq_diff = ((mean - Y_batch)**2)\n",
    "    return lik, sq_diff \n",
    "\n",
    "class CB(object):\n",
    "    def __init__(self, model, assess_model):\n",
    "        self.model = model\n",
    "        self.assess_model = assess_model\n",
    "        self.i = 0\n",
    "        self.t = time.time()\n",
    "        self.train_time = 0\n",
    "        self.ob = []\n",
    "        self.train_lik = []\n",
    "        self.train_rmse = []\n",
    "    def cb(self, x):\n",
    "        self.i += 1\n",
    "        if self.i % 10000 == 0:\n",
    "            # time how long we've be training \n",
    "            self.train_time += time.time() - self.t\n",
    "            self.t = time.time()\n",
    "            \n",
    "            # assess the model on the training data\n",
    "            self.model.set_state(x)\n",
    "            lik, rmse = batch_assess(self.model, self.assess_model, X_val, Y_val)\n",
    "            self.train_lik.append(lik)\n",
    "            self.train_rmse.append(rmse)\n",
    "            \n",
    "            # calculate the objective, averaged over S samples \n",
    "            ob = 0\n",
    "            for _ in range(100):\n",
    "                ob += self.model.compute_log_likelihood()/float(100)\n",
    "            self.ob.append(ob)\n",
    "            \n",
    "            st = 'it: {}, ob: {:.1f}, train lik: {:.4f}, train rmse {:.4f}'\n",
    "            print st.format(self.i, ob, lik, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the sgp for 1 epoch (which is $10^5$ iterations, since the minibatch size is $10^4$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 10000, ob: -8696774181.2, train lik: -8.3145, train rmse 411.2833\n",
      "it: 20000, ob: -8614634441.0, train lik: -8.2654, train rmse 406.2074\n"
     ]
    }
   ],
   "source": [
    "num_iterations = int(2e4)\n",
    "cb_sgp = CB(m_sgp, assess_single_layer)\n",
    "_ = m_sgp.optimize(tf.train.AdamOptimizer(0.01), \n",
    "               maxiter=num_iterations,\n",
    "               callback=cb_sgp.cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgp test lik -8.2373, test rmse 404.4808. Train time: 305.8784\n"
     ]
    }
   ],
   "source": [
    "l, rmse = batch_assess(m_sgp, assess_single_layer, Xs, Ys)\n",
    "print 'sgp test lik {:.4f}, test rmse {:.4f}. Train time: {:.4f}'.format(l, rmse, cb_sgp.train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can modify the DGP class to work with the dataholder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MassiveDataDGP(DGP):\n",
    "    def __init__(self, dataholder, Z, kernels, likelihood, num_latent_Y=1, mean_function=Zero()):\n",
    "        DGP.__init__(self, np.zeros((1, 9)), np.zeros((1, 1)), Z, kernels, likelihood, \n",
    "                     num_latent_Y=num_latent_Y, mean_function=mean_function)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        self.dataholder = dataholder\n",
    "        self.num_data = dataholder.num_data\n",
    "        \n",
    "    def build_likelihood(self):\n",
    "        self.X = self.dataholder[:, :-1]\n",
    "        self.Y = self.dataholder[:, -1, None]        \n",
    "        return DGP.build_likelihood(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a 2 layer DGP model, with the RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels_2 = [RBF(9, ARD=True) + White(9, variance=1e-5), RBF(9, ARD=True, variance=Y_std**2)]\n",
    "m_dgp_2 = MassiveDataDGP(taxi_data, Z.copy(), kernels_2, Gaussian(), mean_function=Constant(Y_mean.copy()))\n",
    "m_dgp_2.likelihood.variance = 0.1*Y_std**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 10000, ob: -7841586373.2, train lik: -7.6574, train rmse 338.2384\n",
      "it: 20000, ob: -7798820462.1, train lik: -7.6387, train rmse 334.2714\n"
     ]
    }
   ],
   "source": [
    "cb_dgp_2 = CB(m_dgp_2, assess_sampled)\n",
    "_ = m_dgp_2.optimize(tf.train.AdamOptimizer(0.01), maxiter=num_iterations, callback=cb_dgp_2.cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgp 2 lik -7.6225, rmse 333.3533. Train time: 1107.6005\n"
     ]
    }
   ],
   "source": [
    "l, rmse = batch_assess(m_dgp_2, assess_sampled, Xs, Ys)\n",
    "print 'dgp 2 lik {:.4f}, rmse {:.4f}. Train time: {:.4f}'.format(l, rmse, cb_dgp_2.train_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the three layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels_3 = [RBF(9, ARD=True) + White(9, variance=1e-5),\n",
    "             RBF(9, ARD=True) + White(9, variance=1e-5), \n",
    "             RBF(9, ARD=True, variance=Y_std**2)]\n",
    "\n",
    "m_dgp_3 = MassiveDataDGP(taxi_data, Z.copy(), kernels_3, Gaussian(), mean_function=Constant(Y_mean.copy()))\n",
    "m_dgp_3.likelihood.variance = 0.01*Y_std**2\n",
    "\n",
    "t = time.time()\n",
    "m_dgp_3.optimize(tf.train.AdamOptimizer(0.01), maxiter=num_iterations)\n",
    "l, rmse = batch_assess(m_dgp_3, assess_sampled, Xs, Ys)\n",
    "print 'dgp 3 lik {:.4f}, rmse {:.4f}. Train time: {:.4f}'.format(l, rmse, time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print m_dgp_2.likelihood\n",
    "print m_sgp.likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
