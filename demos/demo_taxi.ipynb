{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive scale regression: (TODO not yet finished)\n",
    "**Warning:** this dataset will occupy 80GB on your computer. Check that the download location (currently '../data') is appropriate for data this size. You will also need about 20GB of RAM to run this code. \n",
    "\n",
    "The taxi data set consists of 1.21 billion yellow taxi journeys in New York. We got the data from http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n",
    "\n",
    "The processing was as follows:\n",
    "- We extracted the following features: time of day; day of the week; day of the month; month; pick-up latitude and longitude; drop-off latitude and longitude; travel distance; journey time (the target)\n",
    "- We discarded journeys that are less than 10 s or greater than 5 h, or start/end outside the New York region, which we judge to have squared distance less than $5^o$ from the centre of New York\n",
    "- As we read in the data we calculated $\\sum x$ and $\\sum x^2$. These are in the file `taxi_data_stats.p`. We use these for normalizing the data. In the paper we normalise the outputs and restore the scaling, but here we use a mean function and set the variance accordingly. \n",
    "- We shuffled the entire data set (we used a machine with 224GB of memory to do this) and then split the data into 101 files each with $10^7$ lines. We use the first 100 chunks for training and final chunk for testing \n",
    "\n",
    "To use this data set managably on a standard machine we read in two chunks at a time, the second loading asynchronously as the first chunk is used for training. We have a special `DataHolder` class for this  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from GPflow.likelihoods import Gaussian\n",
    "from GPflow.kernels import RBF, White\n",
    "from GPflow.mean_functions import Constant, Zero\n",
    "from GPflow.svgp import SVGP\n",
    "from GPflow.param import DataHolder, Parentable\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from get_data import get_taxi_data, get_taxi_stats\n",
    "\n",
    "from threading import Thread\n",
    "from Queue import Queue\n",
    "\n",
    "from dgp import DGP\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(func, arg, queue):\n",
    "    queue.put(func(arg))\n",
    "\n",
    "class TaxiData(DataHolder):\n",
    "    def __init__(self, minibatch_size=10000):\n",
    "        Parentable.__init__(self)\n",
    "        self._shape = [minibatch_size, 10]\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.counter = 0\n",
    "        self.chunk_counter = 0\n",
    "        \n",
    "        self.num_data = int(10**9)\n",
    "        self.chunk_size = int(10**7)\n",
    "        self.num_chunks = int(self.num_data/self.chunk_size)\n",
    "\n",
    "        self.current_chunk = get_taxi_data(0) # get first chunk\n",
    "        self.chunk_counter += 1\n",
    "        self.start_get_chunk(self.chunk_counter) # start loading next one\n",
    "        \n",
    "        self.X_mean, self.X_std = get_taxi_stats() \n",
    "    \n",
    "    def start_get_chunk(self, i):\n",
    "        self.next_chunk_queued = Queue() \n",
    "        Thread(target=wrapper, args=(get_taxi_data, i, \n",
    "                                     self.next_chunk_queued)).start()\n",
    "    \n",
    "    def get_chunk(self, i):\n",
    "        return self.whiten_X(get_taxi_data(i))\n",
    "    \n",
    "    def whiten_X(self, data):\n",
    "        X = data[:, :-1]\n",
    "        Xw = (X - self.X_mean)/self.X_std\n",
    "        return np.concatenate([Xw, data[:, -1, None]], 1)\n",
    "    \n",
    "    def _get_type(self):\n",
    "        return np.float64\n",
    "\n",
    "    def make_tf_array(self):\n",
    "        self._tf_array = tf.placeholder(dtype=self._get_type(),\n",
    "                                        shape=[None, self._shape[1]],\n",
    "                                        name=self.name)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        return np.prod(self.shape)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "\n",
    "    def __str__(self, prepend='Data:'):\n",
    "        return prepend + \\\n",
    "               '\\033[1m' + self.name + '\\033[0m' + \\\n",
    "               '\\n data not printed!'\n",
    "               \n",
    "    def update_feed_dict(self, key_dict, feed_dict):\n",
    "        if self.counter + self.minibatch_size > self.chunk_size:\n",
    "            self.current_chunk = self.next_chunk_queued.get()\n",
    "            self.chunk_counter = (self.chunk_counter + 1) % self.num_chunks\n",
    "            self.start_get_chunk(self.chunk_counter)\n",
    "            self.counter = 0     \n",
    "       \n",
    "        start = self.counter\n",
    "        end = self.counter + self.minibatch_size\n",
    "        \n",
    "        self.counter += self.minibatch_size\n",
    "        \n",
    "        feed_dict[key_dict[self]] = self.whiten_X(self.current_chunk[start:end, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: taxi_data_shuffled_101.csv\n",
      "Downloaded file: taxi_data_shuffled_101.csv\n"
     ]
    }
   ],
   "source": [
    "taxi_data = TaxiData()\n",
    "test_data = taxi_data.get_chunk(101)\n",
    "Xs, Ys = test_data[:, :-1], test_data[:, -1, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the $10^6$ from the first chunk for kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = kmeans2(taxi_data.current_chunk[:int(1e6), :-1], 100, minit='points')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a single layer model we need to slightly modify the base SVGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassiveDataSVGP(SVGP):\n",
    "    def __init__(self, dataholder, kernel, likelihood, Z, q_diag=False, whiten=True, num_latent=1, mean_function=Zero()):\n",
    "        SVGP.__init__(self, np.zeros((1, 1)), np.zeros((1, 1)), kernel, likelihood, Z, \n",
    "                      q_diag=q_diag, whiten=whiten, num_latent=num_latent)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        self.dataholder = dataholder\n",
    "        self.num_data = dataholder.num_data\n",
    "        \n",
    "    def build_likelihood(self):\n",
    "        self.X = self.dataholder[:, :-1]\n",
    "        self.Y = self.dataholder[:, -1, None]        \n",
    "        return SVGP.build_likelihood(self)\n",
    "        \n",
    "Y_mean, Y_std = np.average(taxi_data.current_chunk[:, -1]), np.std(taxi_data.current_chunk[:, -1])\n",
    "m_sgp = MassiveDataSVGP(taxi_data, RBF(9), Gaussian(), Z.copy(), \n",
    "                        whiten=True, q_diag=False, mean_function=Constant(Y_mean))\n",
    "m_sgp.likelihood.variance = Y_std**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrs13/GPflow/GPflow/transforms.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  result = np.log(np.exp(y - self._lower) - np.ones(1, np_float_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: taxi_data_shuffled_6.csv\n",
      "Downloaded file: taxi_data_shuffled_6.csv\n",
      "Downloading file: taxi_data_shuffled_7.csv\n"
     ]
    }
   ],
   "source": [
    "m_sgp.optimize(tf.train.AdamOptimizer(0.01), maxiter=10000)\n",
    "\n",
    "print m_sgp.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
