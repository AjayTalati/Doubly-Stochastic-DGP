{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP for regression\n",
    "\n",
    "Here we'll show the DGP for regression, using small to medium data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from GPflow.likelihoods import Gaussian\n",
    "from GPflow.kernels import RBF, White\n",
    "from GPflow.mean_functions import Constant\n",
    "from GPflow.sgpr import SGPR, GPRFITC\n",
    "from GPflow.svgp import SVGP\n",
    "from GPflow.gpr import GPR\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "from get_data import get_regression_data\n",
    "from dgp import DGP\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the kin8nm data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 7372, D: 8, Ns: 820\n"
     ]
    }
   ],
   "source": [
    "X, Y, Xs, Ys = get_regression_data('kin8nm', split=0)\n",
    "print 'N: {}, D: {}, Ns: {}'.format(X.shape[0], X.shape[1], Xs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layer models\n",
    "\n",
    "Our baseline model is a sparse GP, but since the dataset is small we can also train without minibatches so we'll also compare to a collapsed sparse GP (with analytically optimal $q(\\mathbf u)$) which is known as SGPR in GPflow terminology, and we'll also cpmpare to FITC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_100 = kmeans2(X, 100, minit='points')[0]\n",
    "\n",
    "D = X.shape[1]\n",
    "Y_mean, Y_std = np.average(Y), np.std(Y) \n",
    "\n",
    "m_sgpr = SGPR(X, Y, RBF(D, variance=Y_std**2), Z_100.copy(), mean_function=Constant(Y_mean))\n",
    "m_svgp = SVGP(X, Y, RBF(D, variance=Y_std**2), Gaussian(), Z_100.copy(), mean_function=Constant(Y_mean), q_diag=False, whiten=True)\n",
    "m_fitc = GPRFITC(X, Y, RBF(D, variance=Y_std**2), Z_100.copy(), mean_function=Constant(Y_mean))\n",
    "\n",
    "for m in [m_sgpr, m_svgp, m_fitc]:\n",
    "    m.mean_function.fixed = True\n",
    "    m.likelihood.variance = 0.1 * Y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGP models\n",
    "\n",
    "We'll include a DGP with a single layer here for comparision. We've used a large minibatch size of $\\text{min}(10000, N)$, but it works just fine for smaller batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dgp(L):\n",
    "    kernels = []\n",
    "    for l in range(L):\n",
    "        kernels.append(RBF(D, lengthscales=1., variance=1.))\n",
    "    mb = 10000 if X.shape[0] > 10000 else None\n",
    "    model = DGP(X, Y, Z_100, kernels, Gaussian(), num_samples=1, minibatch_size=mb)\n",
    "\n",
    "    model.layers[-1].kern.variance = Y_std**2\n",
    "    model.likelihood.variance = Y_std*0.1 \n",
    "    model.layers[-1].mean_function = Constant(Y_mean)\n",
    "    model.layers[-1].mean_function.fixed = True\n",
    "    \n",
    "    # start the inner layers deterministically \n",
    "    for layer in model.layers[:-1]:\n",
    "        layer.q_sqrt = layer.q_sqrt.value * 1e-5\n",
    "    \n",
    "    return model\n",
    "\n",
    "m_dgp1 = make_dgp(1)\n",
    "m_dgp2 = make_dgp(2)\n",
    "m_dgp3 = make_dgp(3)\n",
    "m_dgp4 = make_dgp(4)\n",
    "m_dgp5 = make_dgp(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We'll calculate test rmse and likelihood in batches (so the larger datasets don't cause memory problems)\n",
    "\n",
    "For the DGP models we need to take an average over the samples for the rmse. The `predict_density` function already does this internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/1000.), 1)\n",
    "    lik, sq_diff = [], []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "        l, sq = assess_model(model, X_batch, Y_batch)\n",
    "        lik.append(l)\n",
    "        sq_diff.append(sq)\n",
    "    lik = np.concatenate(lik, 0)\n",
    "    sq_diff = np.array(np.concatenate(sq_diff, 0), dtype=float)\n",
    "    return np.average(lik), np.average(sq_diff)**0.5\n",
    "\n",
    "def assess_single_layer(model, X_batch, Y_batch):\n",
    "    lik = m.predict_density(Xs, Ys)\n",
    "    mean, var = m.predict_y(Xs)\n",
    "    sq_diff = ((mean - Ys)**2)\n",
    "    return lik, sq_diff \n",
    "\n",
    "S = 100\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    lik = m.predict_density(Xs, Ys, S)\n",
    "    mean_samples, var_samples = m.predict_y(Xs, 100)\n",
    "    mean = np.average(mean_samples, 0)\n",
    "    sq_diff = ((mean - Ys)**2)\n",
    "    return lik, sq_diff \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "We'll optimize single layer models and using LFBGS and the dgp models with Adam. It will be interesting to compare the result of `m_svgp` compared to `m_dgp1`: if there is a difference it will be down to the optimizer. \n",
    "\n",
    "We need to take the data in batches to predict for the larger data sets, so we'll define that now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col sgp  lik: 0.9770, rmse: 0.0865. Training time: 13.6648\n",
      "sgp  lik: 0.9670, rmse: 0.0874. Training time: 14.7417\n",
      "fitc  lik: 1.1348, rmse: 0.0826. Training time: 12.8889\n"
     ]
    }
   ],
   "source": [
    "for m, name in zip([m_sgpr, m_svgp, m_fitc], ['col sgp', 'sgp', 'fitc']):\n",
    "    t = time.time()\n",
    "    m.optimize()\n",
    "    lik, rmse = batch_assess(m, assess_single_layer, Xs, Ys)\n",
    "    print '{}  lik: {:.4f}, rmse: {:.4f}. Training time: {:.4f}'.format(name, lik, rmse, time.time() - t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now for the DGP models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgp1  lik: 0.9721, rmse: 0.0870. Training time: 60.8755\n",
      "dgp2  lik: 1.3025, rmse: 0.0659. Training time: 193.0561\n",
      "dgp3  lik: 1.3336, rmse: 0.0638. Training time: 323.3256\n",
      "dgp4  lik: 1.3309, rmse: 0.0642. Training time: 455.6249\n",
      "dgp5  lik: 1.3307, rmse: 0.0641. Training time: 692.9579\n"
     ]
    }
   ],
   "source": [
    "for m, name in zip([m_dgp1, m_dgp2, m_dgp3, m_dgp4, m_dgp5], ['dgp1', 'dgp2', 'dgp3', 'dgp4', 'dgp5']):\n",
    "    t = time.time()\n",
    "    m.optimize(tf.train.AdamOptimizer(0.01), maxiter=5000)\n",
    "    lik, rmse = batch_assess(m, assess_sampled, Xs, Ys)\n",
    "    print '{}  lik: {:.4f}, rmse: {:.4f}. Training time: {:.4f}'.format(name, lik, rmse, time.time() - t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see an improvement in the 2 and 3 layers, but after that additional layers don't add anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
